name: Dataframe Benchmarking

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:  # Allow manual trigger
  schedule:
    # Run weekly on Sundays at 06:00 UTC
    - cron: '0 6 * * 0'

jobs:
  benchmark:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python with uv
      uses: astral-sh/setup-uv@v3
      with:
        enable-cache: true
        cache-dependency-glob: "requirements.in"

    - name: Set up Python
      run: |
        # Automatically detect compatible Python version by testing dependency installation
        # Try default latest Python first, then downgrade up to 2 times if needed
        
        echo "Attempting to find compatible Python version..."
        
        # Try installing default/latest Python
        uv python install
        
        # Get the initial version that was installed
        test_version=$(uv python list | grep -E 'cpython-[0-9]+\.[0-9]+' | grep -v 'pypy' | head -n1 | awk '{print $1}' | sed 's/cpython-//' | cut -d'-' -f1)
        
        for attempt in 1 2 3; do
          echo ""
          echo "Attempt $attempt:"
          echo "Testing Python $test_version..."
          
          # Create a temporary virtual environment with the specific Python version
          # Use attempt number to avoid path collisions
          test_venv="/tmp/test-venv-attempt-$attempt-$$"
          uv venv --python "$test_version" "$test_venv" 2>&1
          source "$test_venv/bin/activate"
          
          # Try to compile requirements (capture output separately for error reporting)
          compile_output=$(mktemp)
          success=false
          if uv pip compile requirements.in > /tmp/requirements-$attempt.txt 2>"$compile_output"; then
            # Compile succeeded, now try to sync
            if uv pip sync /tmp/requirements-$attempt.txt 2>&1; then
              echo "✓ Python $test_version is compatible with all frameworks"
              echo "PYTHON_VERSION=$test_version" >> $GITHUB_ENV
              success=true
            else
              echo "✗ Python $test_version: pip sync failed"
            fi
          else
            echo "✗ Python $test_version: dependency compilation failed"
          fi
          
          # Show the error for debugging if failed
          if [ "$success" = false ]; then
            echo "Error output:"
            cat "$compile_output" 2>/dev/null || true
          fi
          
          # Clean up test venv
          deactivate 2>/dev/null || true
          rm -rf "$test_venv"
          rm -f "$compile_output"
          
          # If successful, we're done
          if [ "$success" = true ]; then
            break
          fi
          
          # If not the last attempt, try downgrading
          if [ $attempt -lt 3 ]; then
            # Parse version and downgrade
            major=$(echo "$test_version" | cut -d. -f1)
            minor=$(echo "$test_version" | cut -d. -f2)
            
            # Downgrade minor version
            new_minor=$((minor - 1))
            
            if [ $new_minor -ge 10 ]; then
              test_version="$major.$new_minor"
              echo "Downgrading to Python $test_version..."
              
              # Install downgraded version
              uv python install "$test_version"
            else
              echo "Cannot downgrade further (reached Python 3.10)"
              exit 1
            fi
          else
            echo "Error: No compatible Python version found after 3 attempts"
            exit 1
          fi
        done
        
        echo ""
        echo "Using Python $PYTHON_VERSION"

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y bc

    - name: Run benchmark suite
      run: |
        # Make run.sh executable
        chmod +x run.sh
        
        # Set Python version for run.sh to use
        # If PYTHON_VERSION was set by the setup step, pin it
        if [ -n "$PYTHON_VERSION" ]; then
          export UV_PYTHON="$PYTHON_VERSION"
          echo "Using Python $PYTHON_VERSION for benchmark suite"
        fi
        
        # Run the benchmark suite
        bash run.sh
      env:
        # Set environment variable to indicate GitHub Actions
        GITHUB_ACTIONS: true

    - name: Display Summary Statistics
      run: |
        echo "::group::Summary Statistics"
        if [ -f "outputs/summary_statistics.md" ]; then
          cat outputs/summary_statistics.md
        else
          echo "Summary statistics file not found"
        fi
        echo "::endgroup::"

    - name: Prepare benchmark artifacts
      run: |
        # Create a comprehensive benchmark report
        mkdir -p benchmark-results
        
        # Copy all output files
        cp -r outputs/* benchmark-results/ 2>/dev/null || true
        cp -r artifacts/* benchmark-results/ 2>/dev/null || true
        
        # Create a summary report
        cat > benchmark-results/README.md << 'EOF'
        # Dataframe Benchmarking Results
        
        This directory contains the complete results from the dataframe benchmarking suite.
        
        ## Files Description
        
        ### Performance Results
        - `comparison_table.csv` - Main performance comparison across frameworks
        - `summary_statistics.csv` - Statistical summary of benchmark results
        - `summary_statistics.md` - Markdown formatted summary statistics
        - `speedup_comparison.csv` - Relative performance metrics (if available)
        - `cache_effectiveness.csv` - Cache performance analysis (if available)
        
        ### Data Verification
        - `hash_comparison.csv` - Hash comparison for result verification
        - `processed_raw_data.csv` - Raw benchmark data
        
        ### Detailed Results
        - `results/` - Individual operation results by framework
        
        ### Profiling Data
        - `pyinstrument/` - Detailed profiling reports (HTML format)
        
        ### Environment Info
        - `requirements_*.txt` - Python environment details and package versions
        
        ## Frameworks Tested
        
        - **pandas**: Traditional dataframe library
        - **fireducks**: High-performance pandas-compatible library  
        - **polars**: Fast dataframes with lazy evaluation
        
        ## Benchmark Operations
        
        The suite tests 23 different operations including:
        - Basic filtering and aggregations
        - Complex multi-table joins
        - Window functions and rolling operations
        - String and datetime operations
        - Statistical computations
        - Large data concatenation and sorting
        
        Generated on: $(date -u '+%Y-%m-%d %H:%M:%S UTC')
        Commit: $GITHUB_SHA
        Workflow: $GITHUB_RUN_ID
        EOF

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ github.run_id }}
        path: benchmark-results/
        retention-days: 30
        compression-level: 6

    - name: Upload profiling reports
      uses: actions/upload-artifact@v4
      with:
        name: profiling-reports-${{ github.run_id }}
        path: outputs/pyinstrument/
        retention-days: 30
        compression-level: 6
      if: always()

    - name: Summary
      run: |
        echo "::notice title=Benchmark Complete::Benchmark suite completed successfully! Check the artifacts for detailed results."
        
        # Display summary statistics if available
        if [ -f "outputs/summary_statistics.md" ]; then
          echo "::group::Summary Statistics"
          cat outputs/summary_statistics.md
          echo "::endgroup::"
        else
          echo "Summary statistics file not found"
        fi