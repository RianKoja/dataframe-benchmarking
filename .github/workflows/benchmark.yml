name: Dataframe Benchmarking

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:  # Allow manual trigger
  schedule:
    # Run weekly on Sundays at 06:00 UTC
    - cron: '0 6 * * 0'

jobs:
  benchmark:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python with uv
      uses: astral-sh/setup-uv@v3
      with:
        enable-cache: true
        cache-dependency-glob: "requirements.in"

    - name: Set up Python
      run: |
        # Automatically detect compatible Python version by testing dependency installation
        # Try default latest Python first, then downgrade up to 2 times if needed
        
        echo "Attempting to find compatible Python version..."
        
        # Try installing default/latest Python
        uv python install
        
        for attempt in 1 2 3; do
          echo ""
          echo "Attempt $attempt:"
          
          # Get current Python version from uv python list
          # Format: "cpython-3.14.0-linux-x86_64-gnu    /path/to/python3.14"
          current_version=$(uv python list | grep -E 'cpython-[0-9]+\.[0-9]+' | grep -v 'pypy' | head -n1 | awk '{print $1}' | sed 's/cpython-//' | cut -d'-' -f1)
          echo "Testing Python $current_version..."
          
          # Try to compile and sync requirements
          # Both compile and sync must succeed for compatibility
          if uv pip compile requirements.in > /tmp/requirements.txt 2>&1 && uv pip sync /tmp/requirements.txt 2>&1; then
            echo "✓ Python $current_version is compatible with all frameworks"
            echo "PYTHON_VERSION=$current_version" >> $GITHUB_ENV
            break
          else
            echo "✗ Python $current_version: dependency installation failed"
            cat /tmp/requirements.txt 2>/dev/null || true
            
            if [ $attempt -lt 3 ]; then
              # Parse version and downgrade
              major=$(echo "$current_version" | cut -d. -f1)
              minor=$(echo "$current_version" | cut -d. -f2)
              
              # Downgrade minor version
              new_minor=$((minor - 1))
              
              if [ $new_minor -ge 10 ]; then
                new_version="$major.$new_minor"
                echo "Downgrading to Python $new_version..."
                
                # Uninstall current version and install downgraded version
                uv python uninstall "$current_version" 2>/dev/null || true
                uv python install "$new_version"
              else
                echo "Cannot downgrade further (reached Python 3.10)"
                exit 1
              fi
            else
              echo "Error: No compatible Python version found after 3 attempts"
              exit 1
            fi
          fi
        done
        
        echo ""
        echo "Using Python $PYTHON_VERSION"

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y bc

    - name: Run benchmark suite
      run: |
        # Make run.sh executable
        chmod +x run.sh
        
        # Run the benchmark suite
        bash run.sh
      env:
        # Set environment variable to indicate GitHub Actions
        GITHUB_ACTIONS: true

    - name: Display Summary Statistics
      run: |
        echo "::group::Summary Statistics"
        if [ -f "outputs/summary_statistics.md" ]; then
          cat outputs/summary_statistics.md
        else
          echo "Summary statistics file not found"
        fi
        echo "::endgroup::"

    - name: Prepare benchmark artifacts
      run: |
        # Create a comprehensive benchmark report
        mkdir -p benchmark-results
        
        # Copy all output files
        cp -r outputs/* benchmark-results/ 2>/dev/null || true
        cp -r artifacts/* benchmark-results/ 2>/dev/null || true
        
        # Create a summary report
        cat > benchmark-results/README.md << 'EOF'
        # Dataframe Benchmarking Results
        
        This directory contains the complete results from the dataframe benchmarking suite.
        
        ## Files Description
        
        ### Performance Results
        - `comparison_table.csv` - Main performance comparison across frameworks
        - `summary_statistics.csv` - Statistical summary of benchmark results
        - `summary_statistics.md` - Markdown formatted summary statistics
        - `speedup_comparison.csv` - Relative performance metrics (if available)
        - `cache_effectiveness.csv` - Cache performance analysis (if available)
        
        ### Data Verification
        - `hash_comparison.csv` - Hash comparison for result verification
        - `processed_raw_data.csv` - Raw benchmark data
        
        ### Detailed Results
        - `results/` - Individual operation results by framework
        
        ### Profiling Data
        - `pyinstrument/` - Detailed profiling reports (HTML format)
        
        ### Environment Info
        - `requirements_*.txt` - Python environment details and package versions
        
        ## Frameworks Tested
        
        - **pandas**: Traditional dataframe library
        - **fireducks**: High-performance pandas-compatible library  
        - **polars**: Fast dataframes with lazy evaluation
        
        ## Benchmark Operations
        
        The suite tests 23 different operations including:
        - Basic filtering and aggregations
        - Complex multi-table joins
        - Window functions and rolling operations
        - String and datetime operations
        - Statistical computations
        - Large data concatenation and sorting
        
        Generated on: $(date -u '+%Y-%m-%d %H:%M:%S UTC')
        Commit: $GITHUB_SHA
        Workflow: $GITHUB_RUN_ID
        EOF

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ github.run_id }}
        path: benchmark-results/
        retention-days: 30
        compression-level: 6

    - name: Upload profiling reports
      uses: actions/upload-artifact@v4
      with:
        name: profiling-reports-${{ github.run_id }}
        path: outputs/pyinstrument/
        retention-days: 30
        compression-level: 6
      if: always()

    - name: Summary
      run: |
        echo "::notice title=Benchmark Complete::Benchmark suite completed successfully! Check the artifacts for detailed results."
        
        # Display summary statistics if available
        if [ -f "outputs/summary_statistics.md" ]; then
          echo "::group::Summary Statistics"
          cat outputs/summary_statistics.md
          echo "::endgroup::"
        else
          echo "Summary statistics file not found"
        fi